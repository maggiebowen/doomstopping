{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WESAD Model Inference Test\n",
                "This notebook verifies the trained model by:\n",
                "1. Generating a fresh feature vector from raw WESAD data (Subject S2).\n",
                "2. Saving it as `artifacts/sample_features.csv`.\n",
                "3. Reloading it and running inference using the saved model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib, json, os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Import WESAD Helper pipelines\n",
                "import sys\n",
                "sys.path.append('../src')\n",
                "from wesad.load_wesad import load_subject_data, extract_labels, extract_chest_signals\n",
                "from wesad.features_eda import extract_eda_features\n",
                "from wesad.features_hrv import extract_hrv_features\n",
                "from wesad.features_acc import extract_acc_features\n",
                "from wesad.normalization import normalize_subjects\n",
                "\n",
                "# Constants\n",
                "FS = 700\n",
                "WINDOW_SAMPLES = 42000  # 60 seconds\n",
                "STEP_SAMPLES = 2100     # 3 seconds overlap step\n",
                "WESAD_PATH = '../data/raw/'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Generate Sample Features (Robust Pipeline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading S2 data...\n",
                        "Extracting features...\n",
                        "Extracted 20 valid windows.\n"
                    ]
                }
            ],
            "source": [
                "# Load Subject S2 (Sample)\n",
                "print(\"Loading S2 data...\")\n",
                "data = load_subject_data(\"S2\", WESAD_PATH)\n",
                "chest_signals = extract_chest_signals(data)\n",
                "labels = extract_labels(data)\n",
                "\n",
                "all_features = []\n",
                "\n",
                "eda_signal = chest_signals['EDA'].flatten()\n",
                "ecg_signal = chest_signals['ECG'].flatten()\n",
                "n_samples = len(labels)\n",
                "\n",
                "# Process just the first 20 windows (approx 2 mins of sliding windows) for speed\n",
                "max_windows = 20\n",
                "window_count = 0\n",
                "\n",
                "print(\"Extracting features...\")\n",
                "for start_idx in range(0, n_samples - WINDOW_SAMPLES + 1, STEP_SAMPLES):\n",
                "    if window_count >= max_windows: break\n",
                "    \n",
                "    end_idx = start_idx + WINDOW_SAMPLES\n",
                "    \n",
                "    # Extract raw window\n",
                "    eda_window = eda_signal[start_idx:end_idx]\n",
                "    ecg_window = ecg_signal[start_idx:end_idx]\n",
                "    acc_window = chest_signals['ACC'][start_idx:end_idx]\n",
                "    labels_window = labels[start_idx:end_idx]\n",
                "    \n",
                "    # Get Label\n",
                "    labels_nonzero = labels_window[labels_window != 0]\n",
                "    if len(labels_nonzero) == 0: continue\n",
                "    window_label = stats.mode(labels_nonzero, keepdims=False)[0]\n",
                "    \n",
                "    # ROBUST FEATURE EXTRACTION\n",
                "    eda_feats = extract_eda_features(eda_window, sampling_rate=FS)\n",
                "    hrv_feats = extract_hrv_features(ecg_window, sampling_rate=FS)\n",
                "    acc_feats = extract_acc_features(acc_window, sampling_rate=FS)\n",
                "    \n",
                "    # Validity Check\n",
                "    if not (eda_feats['valid_eda'] and hrv_feats['valid_hrv']):\n",
                "        continue\n",
                "    \n",
                "    # Clean flags\n",
                "    del eda_feats['valid_eda']\n",
                "    del hrv_feats['valid_hrv']\n",
                "    if 'valid_acc' in acc_feats: del acc_feats['valid_acc']\n",
                "    \n",
                "    # Merge\n",
                "    f = {**eda_feats, **hrv_feats, **acc_feats}\n",
                "    f['label'] = window_label\n",
                "    f['subject'] = 'S2'\n",
                "    all_features.append(f)\n",
                "    window_count += 1\n",
                "\n",
                "df_sample = pd.DataFrame(all_features)\n",
                "print(f\"Extracted {len(df_sample)} valid windows.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Normalization (Subject Baseline)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved normalized sample features to ../artifacts/sample_features.csv\n"
                    ]
                }
            ],
            "source": [
                "# Normalize using S2's own baseline stats from this session\n",
                "# Note: In production you'd load pre-computed baseline stats, but here we calculate on the fly for the test.\n",
                "if os.path.exists('../artifacts/feature_spec.json'):\n",
                "    feature_spec = json.load(open('../artifacts/feature_spec.json'))\n",
                "    feature_cols = feature_spec['feature_cols']\n",
                "    \n",
                "    # Ensure columns exist\n",
                "    missing = [c for c in feature_cols if c not in df_sample.columns]\n",
                "    if missing:\n",
                "        print(f\"Warning: Missing columns {missing}\")\n",
                "        # create empty\n",
                "        for c in missing: df_sample[c] = 0.0\n",
                "    \n",
                "    # Just use built-in normalization for this test DF\n",
                "    # Since S2 is the only subject, it will use its own baseline rows\n",
                "    # Note: Baseline label = 1\n",
                "    X_norm, _ = normalize_subjects(df_sample, df_sample, feature_cols, baseline_label=1)\n",
                "    \n",
                "    # Convert back to DF for saving with names\n",
                "    df_norm = pd.DataFrame(X_norm, columns=feature_cols)\n",
                "    df_norm.to_csv(\"../artifacts/sample_features.csv\", index=False)\n",
                "    print(\"Saved normalized sample features to ../artifacts/sample_features.csv\")\n",
                "else:\n",
                "    print(\"Please generate artifacts/feature_spec.json from the training notebook first!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Inference Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded model artifact (dictionary wrapper detected). Extracting pipeline...\n",
                        "Model uses decision_function (LinearSVC default).\n",
                        "n_windows: 20\n",
                        "pred unique: (array(['other'], dtype=object), array([20]))\n",
                        "proba range/scores: (-1.1873572650404416, -0.015055313141423388)\n"
                    ]
                }
            ],
            "source": [
                "# The test code requested by user\n",
                "if os.path.exists(\"../models/wesad_linear_svm_3class.joblib\") and os.path.exists(\"../artifacts/feature_spec.json\"):\n",
                "    model_container = joblib.load(\"../models/wesad_linear_svm_3class.joblib\")\n",
                "    \n",
                "    # Check if loaded object is the wrapper dictionary\n",
                "    if isinstance(model_container, dict) and 'pipeline' in model_container:\n",
                "        print(\"Loaded model artifact (dictionary wrapper detected). Extracting pipeline...\")\n",
                "        model = model_container['pipeline']\n",
                "    else:\n",
                "        model = model_container\n",
                "        \n",
                "    feat_spec = json.load(open(\"../artifacts/feature_spec.json\"))\n",
                "    feature_cols = feat_spec[\"feature_cols\"]  # ordered list\n",
                "\n",
                "    X = pd.read_csv(\"../artifacts/sample_features.csv\")  # you generate this from WESAD\n",
                "    assert all(c in X.columns for c in feature_cols)\n",
                "\n",
                "    X_ordered = X[feature_cols].to_numpy()\n",
                "    \n",
                "    # Check for probability support\n",
                "    proba = model.predict_proba(X_ordered)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
                "    if proba is None and hasattr(model, \"decision_function\"):\n",
                "        # Fallback to decision function for SVM\n",
                "        print(\"Model uses decision_function (LinearSVC default).\")\n",
                "        proba = model.decision_function(X_ordered)\n",
                "        # If multi-class, this will be shape (n_samples, n_classes). Just taking raw scores.\n",
                "        if len(proba.shape) > 1:\n",
                "             proba = proba[:, 0] # Example for printing\n",
                "        \n",
                "    pred = model.predict(X_ordered)\n",
                "\n",
                "    print(\"n_windows:\", len(X_ordered))\n",
                "    print(\"pred unique:\", np.unique(pred, return_counts=True))\n",
                "    print(\"proba range/scores:\", (float(proba.min()), float(proba.max())) if proba is not None else None)\n",
                "else:\n",
                "    print(\"Model or Feature Spec missing. Please run 01_wesad_stress_model.ipynb first.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "doomstopping",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
