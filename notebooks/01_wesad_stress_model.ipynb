{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WESAD Stress Detection Model\n",
    "\n",
    "Here's the steps of training a stress detection model using the WESAD dataset:\n",
    "1. Load & Explore\n",
    "2. Extract\n",
    "3. Train\n",
    "4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load & Explore WESAD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['_paths', 'EDA', 'BVP', 'TEMP', 'HR', 'ACC', 'IBI', 'tags'])\n",
      "EDA data shape: (31494, 1)\n",
      "IBI head:\n",
      "        t_rel       ibi          time\n",
      "0  14.313155  0.765660  1.495437e+09\n",
      "1  15.203821  0.890666  1.495437e+09\n",
      "2  15.985107  0.781286  1.495437e+09\n",
      "3  16.797644  0.812537  1.495437e+09\n",
      "4  17.578930  0.781286  1.495437e+09\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_subject(subject_dir):\n",
    "    \"\"\"\n",
    "    Load one WESAD subject folder (e.g., .../WESAD/S2/) and return a dict:\n",
    "      signals[\"EDA\"], [\"BVP\"], [\"ACC\"], [\"TEMP\"], [\"HR\"] -> {\"time\": np.ndarray, \"data\": np.ndarray, \"fs\": float, \"start\": float}\n",
    "      signals[\"IBI\"] -> pd.DataFrame with columns [\"t_rel\", \"ibi\", \"time\"]\n",
    "      signals[\"tags\"] -> pd.DataFrame with column [\"time\"]\n",
    "      signals[\"_paths\"] -> {\"subject_dir\": Path, \"e4_dir\": Path}\n",
    "    \"\"\"\n",
    "    subject_dir = Path(subject_dir).expanduser().resolve()\n",
    "    if not subject_dir.exists():\n",
    "        raise FileNotFoundError(f\"Subject directory not found: {subject_dir}\")\n",
    "\n",
    "    # Find *_E4_data (case-insensitive)\n",
    "    e4_candidates = [p for p in subject_dir.iterdir()\n",
    "                     if p.is_dir() and p.name.lower().endswith(\"_e4_data\")]\n",
    "    if not e4_candidates:\n",
    "        contents = [p.name for p in subject_dir.iterdir()]\n",
    "        raise FileNotFoundError(\n",
    "            f\"No '*_E4_data' folder found inside: {subject_dir}\\n\"\n",
    "            f\"Contents: {contents}\"\n",
    "        )\n",
    "    if len(e4_candidates) > 1:\n",
    "        e4_candidates.sort()\n",
    "    e4_dir = e4_candidates[0]\n",
    "\n",
    "    def _load_standard_signal(csv_name, expect_cols=None):\n",
    "        p = e4_dir / csv_name\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing file: {p}\")\n",
    "\n",
    "        raw = pd.read_csv(p, header=None)\n",
    "        if raw.shape[0] < 3:\n",
    "            raise ValueError(f\"File too short: {p} (rows={raw.shape[0]})\")\n",
    "\n",
    "        start = float(raw.iloc[0, 0])\n",
    "        fs = float(raw.iloc[1, 0])\n",
    "        data = raw.iloc[2:].to_numpy(dtype=float)\n",
    "\n",
    "        if expect_cols is not None and data.shape[1] != expect_cols:\n",
    "            raise ValueError(f\"{p.name}: expected {expect_cols} columns, got {data.shape[1]}\")\n",
    "\n",
    "        t = start + np.arange(data.shape[0], dtype=float) / fs\n",
    "        return {\"time\": t, \"data\": data, \"fs\": fs, \"start\": start}\n",
    "\n",
    "    signals = {\n",
    "        \"_paths\": {\"subject_dir\": subject_dir, \"e4_dir\": e4_dir}\n",
    "    }\n",
    "\n",
    "    # Standard sampled signals\n",
    "    signals[\"EDA\"]  = _load_standard_signal(\"EDA.csv\",  expect_cols=1)\n",
    "    signals[\"BVP\"]  = _load_standard_signal(\"BVP.csv\",  expect_cols=1)\n",
    "    signals[\"TEMP\"] = _load_standard_signal(\"TEMP.csv\", expect_cols=1)\n",
    "    signals[\"HR\"]   = _load_standard_signal(\"HR.csv\",   expect_cols=1)\n",
    "    signals[\"ACC\"]  = _load_standard_signal(\"ACC.csv\",  expect_cols=3)\n",
    "\n",
    "    # IBI: first row start unix time; remaining rows are [t_rel, ibi]\n",
    "    ibi_path = e4_dir / \"IBI.csv\"\n",
    "    if not ibi_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {ibi_path}\")\n",
    "\n",
    "    ibi_raw = pd.read_csv(ibi_path, header=None)\n",
    "    if ibi_raw.shape[0] < 2 or ibi_raw.shape[1] < 2:\n",
    "        raise ValueError(f\"Unexpected IBI.csv shape: {ibi_raw.shape} at {ibi_path}\")\n",
    "\n",
    "    ibi_start = float(ibi_raw.iloc[0, 0])\n",
    "    ibi = ibi_raw.iloc[1:, :2].copy()\n",
    "    ibi.columns = [\"t_rel\", \"ibi\"]\n",
    "    ibi[\"t_rel\"] = pd.to_numeric(ibi[\"t_rel\"], errors=\"coerce\")\n",
    "    ibi[\"ibi\"] = pd.to_numeric(ibi[\"ibi\"], errors=\"coerce\")\n",
    "    ibi = ibi.dropna().reset_index(drop=True)\n",
    "    ibi[\"time\"] = ibi_start + ibi[\"t_rel\"].to_numpy(dtype=float)\n",
    "    signals[\"IBI\"] = ibi\n",
    "\n",
    "    # tags: each row is an absolute unix timestamp\n",
    "    tags_path = e4_dir / \"tags.csv\"\n",
    "    if not tags_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {tags_path}\")\n",
    "\n",
    "    tags = pd.read_csv(tags_path, header=None, names=[\"time\"])\n",
    "    tags[\"time\"] = pd.to_numeric(tags[\"time\"], errors=\"coerce\")\n",
    "    tags = tags.dropna().reset_index(drop=True)\n",
    "    signals[\"tags\"] = tags\n",
    "\n",
    "    return signals\n",
    "\n",
    "WESAD_ROOT = Path(\"/Users/maggiebowen/Documents/GitHub/doomstopping/data/raw\")\n",
    "\n",
    "# test calls\n",
    "signals = load_subject(WESAD_ROOT / \"S2\")\n",
    "print(signals.keys())\n",
    "print(\"EDA data shape:\", signals[\"EDA\"][\"data\"].shape)\n",
    "print(\"IBI head:\\n\", signals[\"IBI\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label exploration and mapping\n",
    "Quick helpers to inspect WESAD label segments and map them to tasks without touching existing cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label counts: {0: 2142701, 1: 800800, 2: 430500, 3: 253400, 4: 537599, 6: 45500, 7: 44800}\n"
     ]
    }
   ],
   "source": [
    "# Inspect one subject's label distribution (labels are aligned to the chest sample rate)\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_pickle_subject(subject_id, root=None):\n",
    "    root_path = Path(root) if root is not None else Path('/Users/maggiebowen/Documents/GitHub/doomstopping/data/raw')\n",
    "    pkl_path = root_path / subject_id / f'{subject_id}.pkl'\n",
    "    with pkl_path.open('rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "s2 = load_pickle_subject('S2')\n",
    "labels = s2['label'].astype(int)\n",
    "vals, counts = np.unique(labels, return_counts=True)\n",
    "print('Label counts:', dict(zip(vals.tolist(), counts.tolist())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 segments\n",
      "00: label=0 start=0 len=214583 dur_s=306.5\n",
      "01: label=1 start=214583 len=800800 dur_s=1144.0\n",
      "02: label=0 start=1015383 len=576099 dur_s=823.0\n",
      "03: label=2 start=1591482 len=430500 dur_s=615.0\n",
      "04: label=0 start=2021982 len=190401 dur_s=272.0\n",
      "05: label=6 start=2212383 len=45500 dur_s=65.0\n",
      "06: label=0 start=2257883 len=610400 dur_s=872.0\n",
      "07: label=4 start=2868283 len=273699 dur_s=391.0\n",
      "08: label=0 start=3141982 len=192501 dur_s=275.0\n",
      "09: label=3 start=3334483 len=253400 dur_s=362.0\n",
      "10: label=0 start=3587883 len=100800 dur_s=144.0\n",
      "11: label=7 start=3688683 len=44800 dur_s=64.0\n",
      "12: label=0 start=3733483 len=114100 dur_s=163.0\n",
      "13: label=4 start=3847583 len=263900 dur_s=377.0\n",
      "14: label=0 start=4111483 len=143817 dur_s=205.5\n"
     ]
    }
   ],
   "source": [
    "# Run-length encode labels to see segment order and durations (seconds assume 700 Hz chest rate)\n",
    "def label_segments(labels, fs=700):\n",
    "    segments = []\n",
    "    start = 0\n",
    "    cur = int(labels[0])\n",
    "    for idx, val in enumerate(labels[1:], 1):\n",
    "        v = int(val)\n",
    "        if v != cur:\n",
    "            seg_len = idx - start\n",
    "            segments.append({'label': cur, 'start_idx': start, 'len_samples': seg_len, 'dur_s': seg_len / fs})\n",
    "            cur, start = v, idx\n",
    "    seg_len = len(labels) - start\n",
    "    segments.append({'label': cur, 'start_idx': start, 'len_samples': seg_len, 'dur_s': seg_len / fs})\n",
    "    return segments\n",
    "\n",
    "segments = label_segments(labels)\n",
    "print(f'Found {len(segments)} segments')\n",
    "for i, seg in enumerate(segments):\n",
    "    print(f\"{i:02d}: label={seg['label']} start={seg['start_idx']} len={seg['len_samples']} dur_s={seg['dur_s']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WESAD label mapping (from official schedule):**\n",
    "- 1 = Baseline\n",
    "- 2 = TSST stress\n",
    "- 3 = Amusement\n",
    "- 4 = Meditation\n",
    "- 6 = sRead (self-report/reading)\n",
    "- 7 = fRead (final reading)\n",
    "- 0 = transition / not-worn / between blocks\n",
    "\n",
    "Use the quest CSV in each subject folder (e.g., `data/raw/S2/S2_quest.csv`) to line up minute-level START/END times with the run-length segments above. For binary stress modeling, collapse to stress=2 vs non-stress={1,3,4,6,7} and drop/ignore 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch label summary (all subjects)\n",
    "Run the reusable script and load its outputs for quick inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maggiebowen/miniconda3/envs/doomstopping/bin/python: can't open file '/Users/maggiebowen/Documents/GitHub/doomstopping/notebooks/../scripts/summarize_wesad.py': [Errno 2] No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['/Users/maggiebowen/miniconda3/envs/doomstopping/bin/python', '../scripts/summarize_wesad.py']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generate per-subject label summaries to data/processed/\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../scripts/summarize_wesad.py\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/doomstopping/lib/python3.11/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['/Users/maggiebowen/miniconda3/envs/doomstopping/bin/python', '../scripts/summarize_wesad.py']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "# Generate per-subject label summaries to data/processed/\n",
    "import subprocess, sys\n",
    "subprocess.run([sys.executable, '../scripts/summarize_wesad.py'], check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preview summary CSVs without pandas\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "counts_path = Path('../data/processed/wesad_label_counts.csv')\n",
    "segments_path = Path('../data/processed/wesad_label_segments.csv')\n",
    "\n",
    "def head_csv(path, n=5):\n",
    "    rows = []\n",
    "    with path.open() as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            rows.append(row)\n",
    "            if i >= n:\n",
    "                break\n",
    "    return rows\n",
    "\n",
    "print('Counts:', counts_path)\n",
    "for row in head_csv(counts_path):\n",
    "    print(row)\n",
    "\n",
    "print('\n",
    "Segments:', segments_path)\n",
    "for row in head_csv(segments_path):\n",
    "    print(row)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "doomstopping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
